{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a Model\n",
    "\n",
    "The goal of this notebook is to choose the best model for predicting scores of NCAA Tournament games. We will use the training data to run several machine learning models for our data. Finally, using the results, we select a modle to use for our NCAA Tournament predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import collegebasketball as cbb\n",
    "cbb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "First, we need to load the training data. This data set was built by creating feature vectors, removing games that don't accurately represent the NCAA Tournament games and reducing the number of features to a manageable number. More information about how this was done can be found in the Creating the Training Data, Covariate Shift Analysis and Feature Reduction notebooks. We'll split this data into training and test sets (with an even proportion of regular season and tournament games in each) so that we can tune the models with the training set before finally testing them with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games in training data: 14650\n",
      "Regular season games: 14050, Tournament games: 600\n"
     ]
    }
   ],
   "source": [
    "# Load the csv files that contain the scores/kenpom data\n",
    "path = '../../Data/Training/training_feat_reduced.csv'\n",
    "training = pd.read_csv(path)\n",
    "season = cbb.filter_tournament(training, drop=True)\n",
    "march = cbb.filter_tournament(training)\n",
    "exclude_cols = ['Favored', 'Underdog', 'Year', 'Tournament', 'Label']\n",
    "\n",
    "# Get a sense for the size of each data set\n",
    "print('Games in training data: {}'.format(len(training)))\n",
    "print('Regular season games: {0}, Tournament games: {1}'.format(len(season), len(march)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games in training set: 10987\n",
      "Games in test set: 3663\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets with an equal proportion of actual tournament games in each\n",
    "season_train, season_test = train_test_split(season, random_state=77)\n",
    "march_train, march_test = train_test_split(march, random_state=77)\n",
    "\n",
    "train = pd.concat([season_train, march_train])\n",
    "test = pd.concat([season_test, march_test])\n",
    "\n",
    "print('Games in training set: {}'.format(len(train)))\n",
    "print('Games in test set: {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Tuning\n",
    "\n",
    "To start with, we'll try to tune some parameters for a logistic regression model. We'll try both Ridge and Lasso regression with different values for the `C` parameter (where a smaller value imposes a harsher penalty). To compare the different models, we'll run five fold cross validation on the test set and compare the average f1 and AUC scores across the different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Run 1</th>\n",
       "      <th>Run 2</th>\n",
       "      <th>Run 3</th>\n",
       "      <th>Run 4</th>\n",
       "      <th>Run 5</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_reg_l1_0.1</td>\n",
       "      <td>0.302714</td>\n",
       "      <td>0.340249</td>\n",
       "      <td>0.307851</td>\n",
       "      <td>0.320487</td>\n",
       "      <td>0.294235</td>\n",
       "      <td>0.313107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>log_reg_l1_1</td>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.371957</td>\n",
       "      <td>0.364694</td>\n",
       "      <td>0.345296</td>\n",
       "      <td>0.353488</td>\n",
       "      <td>0.360239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>log_reg_l1_10</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.377176</td>\n",
       "      <td>0.372361</td>\n",
       "      <td>0.345560</td>\n",
       "      <td>0.363299</td>\n",
       "      <td>0.364547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>log_reg_l1_50</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.378119</td>\n",
       "      <td>0.372361</td>\n",
       "      <td>0.349084</td>\n",
       "      <td>0.368178</td>\n",
       "      <td>0.366276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>log_reg_l1_100</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.378119</td>\n",
       "      <td>0.372004</td>\n",
       "      <td>0.349421</td>\n",
       "      <td>0.366327</td>\n",
       "      <td>0.365902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>log_reg_l2_0.1</td>\n",
       "      <td>0.305931</td>\n",
       "      <td>0.326784</td>\n",
       "      <td>0.319672</td>\n",
       "      <td>0.304663</td>\n",
       "      <td>0.289133</td>\n",
       "      <td>0.309237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>log_reg_l2_1</td>\n",
       "      <td>0.364356</td>\n",
       "      <td>0.368473</td>\n",
       "      <td>0.359726</td>\n",
       "      <td>0.340216</td>\n",
       "      <td>0.349953</td>\n",
       "      <td>0.356545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>log_reg_l2_10</td>\n",
       "      <td>0.377691</td>\n",
       "      <td>0.375969</td>\n",
       "      <td>0.365759</td>\n",
       "      <td>0.345967</td>\n",
       "      <td>0.366512</td>\n",
       "      <td>0.366379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>log_reg_l2_50</td>\n",
       "      <td>0.378537</td>\n",
       "      <td>0.374156</td>\n",
       "      <td>0.364694</td>\n",
       "      <td>0.350097</td>\n",
       "      <td>0.367688</td>\n",
       "      <td>0.367034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>log_reg_l2_100</td>\n",
       "      <td>0.381413</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.369942</td>\n",
       "      <td>0.345631</td>\n",
       "      <td>0.364815</td>\n",
       "      <td>0.367723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classifier     Run 1     Run 2     Run 3     Run 4     Run 5   Average\n",
       "0  log_reg_l1_0.1  0.302714  0.340249  0.307851  0.320487  0.294235  0.313107\n",
       "1    log_reg_l1_1  0.365759  0.371957  0.364694  0.345296  0.353488  0.360239\n",
       "2   log_reg_l1_10  0.364341  0.377176  0.372361  0.345560  0.363299  0.364547\n",
       "3   log_reg_l1_50  0.363636  0.378119  0.372361  0.349084  0.368178  0.366276\n",
       "4  log_reg_l1_100  0.363636  0.378119  0.372004  0.349421  0.366327  0.365902\n",
       "5  log_reg_l2_0.1  0.305931  0.326784  0.319672  0.304663  0.289133  0.309237\n",
       "6    log_reg_l2_1  0.364356  0.368473  0.359726  0.340216  0.349953  0.356545\n",
       "7   log_reg_l2_10  0.377691  0.375969  0.365759  0.345967  0.366512  0.366379\n",
       "8   log_reg_l2_50  0.378537  0.374156  0.364694  0.350097  0.367688  0.367034\n",
       "9  log_reg_l2_100  0.381413  0.376812  0.369942  0.345631  0.364815  0.367723"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalties = ['l1', 'l2']\n",
    "c_values = [0.1, 1, 10, 50, 100]\n",
    "\n",
    "cl_names = list()\n",
    "clfs = list()\n",
    "for p in penalties:\n",
    "    for c in c_values:\n",
    "        cl_names.append('log_reg_{0}_{1}'.format(p, c))\n",
    "        clfs.append(LogisticRegression(penalty=p, C=c, solver='liblinear', random_state=77))\n",
    "\n",
    "cbb.cross_val(train, exclude_cols, clfs, cl_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Run 1</th>\n",
       "      <th>Run 2</th>\n",
       "      <th>Run 3</th>\n",
       "      <th>Run 4</th>\n",
       "      <th>Run 5</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_reg_l1_0.1</td>\n",
       "      <td>0.724025</td>\n",
       "      <td>0.699240</td>\n",
       "      <td>0.691655</td>\n",
       "      <td>0.702440</td>\n",
       "      <td>0.694534</td>\n",
       "      <td>0.702379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>log_reg_l1_1</td>\n",
       "      <td>0.728627</td>\n",
       "      <td>0.708164</td>\n",
       "      <td>0.703997</td>\n",
       "      <td>0.700754</td>\n",
       "      <td>0.698454</td>\n",
       "      <td>0.707999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>log_reg_l1_10</td>\n",
       "      <td>0.728233</td>\n",
       "      <td>0.708317</td>\n",
       "      <td>0.704799</td>\n",
       "      <td>0.700142</td>\n",
       "      <td>0.698375</td>\n",
       "      <td>0.707973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>log_reg_l1_50</td>\n",
       "      <td>0.728160</td>\n",
       "      <td>0.709280</td>\n",
       "      <td>0.704842</td>\n",
       "      <td>0.699778</td>\n",
       "      <td>0.698279</td>\n",
       "      <td>0.708068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>log_reg_l1_100</td>\n",
       "      <td>0.728132</td>\n",
       "      <td>0.709298</td>\n",
       "      <td>0.704845</td>\n",
       "      <td>0.699714</td>\n",
       "      <td>0.698397</td>\n",
       "      <td>0.708077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>log_reg_l2_0.1</td>\n",
       "      <td>0.720457</td>\n",
       "      <td>0.695317</td>\n",
       "      <td>0.691627</td>\n",
       "      <td>0.701052</td>\n",
       "      <td>0.688896</td>\n",
       "      <td>0.699470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>log_reg_l2_1</td>\n",
       "      <td>0.728528</td>\n",
       "      <td>0.706925</td>\n",
       "      <td>0.703481</td>\n",
       "      <td>0.701667</td>\n",
       "      <td>0.696334</td>\n",
       "      <td>0.707387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>log_reg_l2_10</td>\n",
       "      <td>0.728579</td>\n",
       "      <td>0.707850</td>\n",
       "      <td>0.704614</td>\n",
       "      <td>0.699892</td>\n",
       "      <td>0.697004</td>\n",
       "      <td>0.707588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>log_reg_l2_50</td>\n",
       "      <td>0.728820</td>\n",
       "      <td>0.708269</td>\n",
       "      <td>0.704568</td>\n",
       "      <td>0.699576</td>\n",
       "      <td>0.696844</td>\n",
       "      <td>0.707615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>log_reg_l2_100</td>\n",
       "      <td>0.728501</td>\n",
       "      <td>0.708050</td>\n",
       "      <td>0.705009</td>\n",
       "      <td>0.699637</td>\n",
       "      <td>0.696961</td>\n",
       "      <td>0.707632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classifier     Run 1     Run 2     Run 3     Run 4     Run 5   Average\n",
       "0  log_reg_l1_0.1  0.724025  0.699240  0.691655  0.702440  0.694534  0.702379\n",
       "1    log_reg_l1_1  0.728627  0.708164  0.703997  0.700754  0.698454  0.707999\n",
       "2   log_reg_l1_10  0.728233  0.708317  0.704799  0.700142  0.698375  0.707973\n",
       "3   log_reg_l1_50  0.728160  0.709280  0.704842  0.699778  0.698279  0.708068\n",
       "4  log_reg_l1_100  0.728132  0.709298  0.704845  0.699714  0.698397  0.708077\n",
       "5  log_reg_l2_0.1  0.720457  0.695317  0.691627  0.701052  0.688896  0.699470\n",
       "6    log_reg_l2_1  0.728528  0.706925  0.703481  0.701667  0.696334  0.707387\n",
       "7   log_reg_l2_10  0.728579  0.707850  0.704614  0.699892  0.697004  0.707588\n",
       "8   log_reg_l2_50  0.728820  0.708269  0.704568  0.699576  0.696844  0.707615\n",
       "9  log_reg_l2_100  0.728501  0.708050  0.705009  0.699637  0.696961  0.707632"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbb.cross_val(train, exclude_cols, clfs, cl_names, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cross validation results above, I think both types of regularization penalties work well with a C value of 10. Since the chance of an upset is extremely variable, we want a robust model with a harsh enough regularization penalty, so I don't want the C value to be too high, but the f1 score does appear to increase as we increase C. I think 10 is a good middle ground and what we will use moving forward.\n",
    "\n",
    "Below, we run train a model with each set of parameters on regular season data from the training data and evaluate them with the tournament data from the training set, just to see how well they seem to be able to predict tournament games. We can see that the performance is very similar between the two models, but there is a slight advantage for the Ridge Regression model.\n",
    "\n",
    "In the end, the best logistic regression parameters for me are to use l2 regularization with a C value of 10. I think this will be the most robust logistic regression model we've tested here and we will compare it to the other algorithms later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L2 Penalty</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.229358</td>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.734779</td>\n",
       "      <td>0.163327</td>\n",
       "      <td>0.757778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1 Penalty</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.229358</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.732008</td>\n",
       "      <td>0.164102</td>\n",
       "      <td>0.755556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classifier  Precision    Recall        F1       AUC     Brier  Accuracy\n",
       "0  L2 Penalty   0.500000  0.229358  0.314465  0.734779  0.163327  0.757778\n",
       "1  L1 Penalty   0.490196  0.229358  0.312500  0.732008  0.164102  0.755556"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l2 = LogisticRegression(penalty='l2', C=10, solver='liblinear', random_state=77)\n",
    "clf_l1 = LogisticRegression(penalty='l1', C=10, solver='liblinear', random_state=77)\n",
    "cbb.evaluate(season_train, march_train, exclude_cols, [clf_l2, clf_l1], ['L2 Penalty', 'L1 Penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Tuning\n",
    "\n",
    "Next, we will follow a similar process to tune a random forest model. This time, we will be trying different values for the minimum number of samples to split, the maximum depth of trees and the maximum features used per tree. For reference, the default values in scikit-learn are 2, None and 'sqrt' respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Run 1</th>\n",
       "      <th>Run 2</th>\n",
       "      <th>Run 3</th>\n",
       "      <th>Run 4</th>\n",
       "      <th>Run 5</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_2_None_sqrt</td>\n",
       "      <td>0.286229</td>\n",
       "      <td>0.288952</td>\n",
       "      <td>0.246068</td>\n",
       "      <td>0.257353</td>\n",
       "      <td>0.282741</td>\n",
       "      <td>0.272269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_2_None_log2</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.242537</td>\n",
       "      <td>0.260223</td>\n",
       "      <td>0.267951</td>\n",
       "      <td>0.265925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_2_None_10</td>\n",
       "      <td>0.294380</td>\n",
       "      <td>0.297911</td>\n",
       "      <td>0.271493</td>\n",
       "      <td>0.289331</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.288638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_2_25_sqrt</td>\n",
       "      <td>0.296029</td>\n",
       "      <td>0.303309</td>\n",
       "      <td>0.246296</td>\n",
       "      <td>0.283623</td>\n",
       "      <td>0.275261</td>\n",
       "      <td>0.280904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_2_25_log2</td>\n",
       "      <td>0.256030</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.250464</td>\n",
       "      <td>0.258364</td>\n",
       "      <td>0.287958</td>\n",
       "      <td>0.265280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_2_25_10</td>\n",
       "      <td>0.298748</td>\n",
       "      <td>0.302703</td>\n",
       "      <td>0.264253</td>\n",
       "      <td>0.284946</td>\n",
       "      <td>0.291312</td>\n",
       "      <td>0.288392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_2_15_sqrt</td>\n",
       "      <td>0.261860</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.253788</td>\n",
       "      <td>0.254302</td>\n",
       "      <td>0.241224</td>\n",
       "      <td>0.256081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_2_15_log2</td>\n",
       "      <td>0.259716</td>\n",
       "      <td>0.264561</td>\n",
       "      <td>0.223735</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.246550</td>\n",
       "      <td>0.246772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_2_15_10</td>\n",
       "      <td>0.269017</td>\n",
       "      <td>0.293785</td>\n",
       "      <td>0.257223</td>\n",
       "      <td>0.250235</td>\n",
       "      <td>0.286964</td>\n",
       "      <td>0.271445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_5_None_sqrt</td>\n",
       "      <td>0.285453</td>\n",
       "      <td>0.296846</td>\n",
       "      <td>0.260073</td>\n",
       "      <td>0.271809</td>\n",
       "      <td>0.268864</td>\n",
       "      <td>0.276609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_5_None_log2</td>\n",
       "      <td>0.287020</td>\n",
       "      <td>0.278937</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.273234</td>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.271489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_5_None_10</td>\n",
       "      <td>0.290614</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.267745</td>\n",
       "      <td>0.268877</td>\n",
       "      <td>0.295533</td>\n",
       "      <td>0.285160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_5_25_sqrt</td>\n",
       "      <td>0.286232</td>\n",
       "      <td>0.281664</td>\n",
       "      <td>0.256176</td>\n",
       "      <td>0.271809</td>\n",
       "      <td>0.288428</td>\n",
       "      <td>0.276862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_5_25_log2</td>\n",
       "      <td>0.267528</td>\n",
       "      <td>0.278095</td>\n",
       "      <td>0.252572</td>\n",
       "      <td>0.263207</td>\n",
       "      <td>0.266430</td>\n",
       "      <td>0.265566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_5_25_10</td>\n",
       "      <td>0.291403</td>\n",
       "      <td>0.295413</td>\n",
       "      <td>0.266907</td>\n",
       "      <td>0.270221</td>\n",
       "      <td>0.275983</td>\n",
       "      <td>0.279985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_5_15_sqrt</td>\n",
       "      <td>0.260704</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.253092</td>\n",
       "      <td>0.259048</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.261465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_5_15_log2</td>\n",
       "      <td>0.248776</td>\n",
       "      <td>0.267198</td>\n",
       "      <td>0.221352</td>\n",
       "      <td>0.259078</td>\n",
       "      <td>0.246097</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_5_15_10</td>\n",
       "      <td>0.274328</td>\n",
       "      <td>0.279734</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>0.271698</td>\n",
       "      <td>0.277087</td>\n",
       "      <td>0.272629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_10_None_sqrt</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.290566</td>\n",
       "      <td>0.256267</td>\n",
       "      <td>0.274074</td>\n",
       "      <td>0.278369</td>\n",
       "      <td>0.272487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_10_None_log2</td>\n",
       "      <td>0.275992</td>\n",
       "      <td>0.279468</td>\n",
       "      <td>0.239151</td>\n",
       "      <td>0.265907</td>\n",
       "      <td>0.265948</td>\n",
       "      <td>0.265293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_10_None_10</td>\n",
       "      <td>0.283636</td>\n",
       "      <td>0.285182</td>\n",
       "      <td>0.264081</td>\n",
       "      <td>0.270621</td>\n",
       "      <td>0.291883</td>\n",
       "      <td>0.279081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_10_25_sqrt</td>\n",
       "      <td>0.272642</td>\n",
       "      <td>0.277831</td>\n",
       "      <td>0.254884</td>\n",
       "      <td>0.280184</td>\n",
       "      <td>0.276351</td>\n",
       "      <td>0.272378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_10_25_log2</td>\n",
       "      <td>0.271442</td>\n",
       "      <td>0.281637</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.257388</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>0.262622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_10_25_10</td>\n",
       "      <td>0.295023</td>\n",
       "      <td>0.295814</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.280961</td>\n",
       "      <td>0.298533</td>\n",
       "      <td>0.286014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rf_10_15_sqrt</td>\n",
       "      <td>0.263415</td>\n",
       "      <td>0.262948</td>\n",
       "      <td>0.246602</td>\n",
       "      <td>0.231306</td>\n",
       "      <td>0.258780</td>\n",
       "      <td>0.252610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf_10_15_log2</td>\n",
       "      <td>0.243028</td>\n",
       "      <td>0.256513</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.256207</td>\n",
       "      <td>0.234957</td>\n",
       "      <td>0.241384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rf_10_15_10</td>\n",
       "      <td>0.266921</td>\n",
       "      <td>0.286271</td>\n",
       "      <td>0.262452</td>\n",
       "      <td>0.252427</td>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.268899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier     Run 1     Run 2     Run 3     Run 4     Run 5  \\\n",
       "0    rf_2_None_sqrt  0.286229  0.288952  0.246068  0.257353  0.282741   \n",
       "1    rf_2_None_log2  0.284404  0.274510  0.242537  0.260223  0.267951   \n",
       "2      rf_2_None_10  0.294380  0.297911  0.271493  0.289331  0.290076   \n",
       "3      rf_2_25_sqrt  0.296029  0.303309  0.246296  0.283623  0.275261   \n",
       "4      rf_2_25_log2  0.256030  0.273585  0.250464  0.258364  0.287958   \n",
       "5        rf_2_25_10  0.298748  0.302703  0.264253  0.284946  0.291312   \n",
       "6      rf_2_15_sqrt  0.261860  0.269231  0.253788  0.254302  0.241224   \n",
       "7      rf_2_15_log2  0.259716  0.264561  0.223735  0.239300  0.246550   \n",
       "8        rf_2_15_10  0.269017  0.293785  0.257223  0.250235  0.286964   \n",
       "9    rf_5_None_sqrt  0.285453  0.296846  0.260073  0.271809  0.268864   \n",
       "10   rf_5_None_log2  0.287020  0.278937  0.253968  0.273234  0.264286   \n",
       "11     rf_5_None_10  0.290614  0.303030  0.267745  0.268877  0.295533   \n",
       "12     rf_5_25_sqrt  0.286232  0.281664  0.256176  0.271809  0.288428   \n",
       "13     rf_5_25_log2  0.267528  0.278095  0.252572  0.263207  0.266430   \n",
       "14       rf_5_25_10  0.291403  0.295413  0.266907  0.270221  0.275983   \n",
       "15     rf_5_15_sqrt  0.260704  0.293103  0.253092  0.259048  0.241379   \n",
       "16     rf_5_15_log2  0.248776  0.267198  0.221352  0.259078  0.246097   \n",
       "17       rf_5_15_10  0.274328  0.279734  0.260300  0.271698  0.277087   \n",
       "18  rf_10_None_sqrt  0.263158  0.290566  0.256267  0.274074  0.278369   \n",
       "19  rf_10_None_log2  0.275992  0.279468  0.239151  0.265907  0.265948   \n",
       "20    rf_10_None_10  0.283636  0.285182  0.264081  0.270621  0.291883   \n",
       "21    rf_10_25_sqrt  0.272642  0.277831  0.254884  0.280184  0.276351   \n",
       "22    rf_10_25_log2  0.271442  0.281637  0.231884  0.257388  0.270758   \n",
       "23      rf_10_25_10  0.295023  0.295814  0.259740  0.280961  0.298533   \n",
       "24    rf_10_15_sqrt  0.263415  0.262948  0.246602  0.231306  0.258780   \n",
       "25    rf_10_15_log2  0.243028  0.256513  0.216216  0.256207  0.234957   \n",
       "26      rf_10_15_10  0.266921  0.286271  0.262452  0.252427  0.276423   \n",
       "\n",
       "     Average  \n",
       "0   0.272269  \n",
       "1   0.265925  \n",
       "2   0.288638  \n",
       "3   0.280904  \n",
       "4   0.265280  \n",
       "5   0.288392  \n",
       "6   0.256081  \n",
       "7   0.246772  \n",
       "8   0.271445  \n",
       "9   0.276609  \n",
       "10  0.271489  \n",
       "11  0.285160  \n",
       "12  0.276862  \n",
       "13  0.265566  \n",
       "14  0.279985  \n",
       "15  0.261465  \n",
       "16  0.248500  \n",
       "17  0.272629  \n",
       "18  0.272487  \n",
       "19  0.265293  \n",
       "20  0.279081  \n",
       "21  0.272378  \n",
       "22  0.262622  \n",
       "23  0.286014  \n",
       "24  0.252610  \n",
       "25  0.241384  \n",
       "26  0.268899  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_samples_splits = [2, 5, 10]\n",
    "max_depths = [None, 25, 15]\n",
    "max_features = ['sqrt', 'log2', 10]\n",
    "\n",
    "cl_names = list()\n",
    "clfs = list()\n",
    "for s in min_samples_splits:\n",
    "    for d in max_depths:\n",
    "        for f in max_features:\n",
    "            cl_names.append('rf_{0}_{1}_{2}'.format(s, d, f))\n",
    "            clfs.append(RandomForestClassifier(min_samples_split=s, max_depth=d, max_features=f, \n",
    "                                               n_jobs=-1, random_state=77))\n",
    "\n",
    "cross_val_result = cbb.cross_val(train, exclude_cols, clfs, cl_names)\n",
    "cross_val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Run 1</th>\n",
       "      <th>Run 2</th>\n",
       "      <th>Run 3</th>\n",
       "      <th>Run 4</th>\n",
       "      <th>Run 5</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_2_None_sqrt</td>\n",
       "      <td>0.583218</td>\n",
       "      <td>0.578027</td>\n",
       "      <td>0.563582</td>\n",
       "      <td>0.575798</td>\n",
       "      <td>0.569615</td>\n",
       "      <td>0.574048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_2_None_log2</td>\n",
       "      <td>0.574066</td>\n",
       "      <td>0.574220</td>\n",
       "      <td>0.564018</td>\n",
       "      <td>0.573494</td>\n",
       "      <td>0.568070</td>\n",
       "      <td>0.570774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_2_None_10</td>\n",
       "      <td>0.587130</td>\n",
       "      <td>0.585437</td>\n",
       "      <td>0.566015</td>\n",
       "      <td>0.580056</td>\n",
       "      <td>0.579016</td>\n",
       "      <td>0.579531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_2_25_sqrt</td>\n",
       "      <td>0.582239</td>\n",
       "      <td>0.580290</td>\n",
       "      <td>0.567900</td>\n",
       "      <td>0.584945</td>\n",
       "      <td>0.573208</td>\n",
       "      <td>0.577716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_2_25_log2</td>\n",
       "      <td>0.576208</td>\n",
       "      <td>0.572987</td>\n",
       "      <td>0.561835</td>\n",
       "      <td>0.575825</td>\n",
       "      <td>0.567494</td>\n",
       "      <td>0.570870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_2_25_10</td>\n",
       "      <td>0.585995</td>\n",
       "      <td>0.582536</td>\n",
       "      <td>0.563105</td>\n",
       "      <td>0.581095</td>\n",
       "      <td>0.577646</td>\n",
       "      <td>0.578076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf_2_15_sqrt</td>\n",
       "      <td>0.598159</td>\n",
       "      <td>0.596776</td>\n",
       "      <td>0.584242</td>\n",
       "      <td>0.597314</td>\n",
       "      <td>0.579315</td>\n",
       "      <td>0.591161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf_2_15_log2</td>\n",
       "      <td>0.606509</td>\n",
       "      <td>0.599328</td>\n",
       "      <td>0.583712</td>\n",
       "      <td>0.597333</td>\n",
       "      <td>0.584062</td>\n",
       "      <td>0.594189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf_2_15_10</td>\n",
       "      <td>0.605062</td>\n",
       "      <td>0.604182</td>\n",
       "      <td>0.586051</td>\n",
       "      <td>0.597718</td>\n",
       "      <td>0.588239</td>\n",
       "      <td>0.596250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf_5_None_sqrt</td>\n",
       "      <td>0.588709</td>\n",
       "      <td>0.590061</td>\n",
       "      <td>0.574136</td>\n",
       "      <td>0.582569</td>\n",
       "      <td>0.575039</td>\n",
       "      <td>0.582103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf_5_None_log2</td>\n",
       "      <td>0.582678</td>\n",
       "      <td>0.585487</td>\n",
       "      <td>0.565741</td>\n",
       "      <td>0.582234</td>\n",
       "      <td>0.574046</td>\n",
       "      <td>0.578037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf_5_None_10</td>\n",
       "      <td>0.586709</td>\n",
       "      <td>0.588021</td>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.585916</td>\n",
       "      <td>0.586109</td>\n",
       "      <td>0.584494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf_5_25_sqrt</td>\n",
       "      <td>0.587815</td>\n",
       "      <td>0.593396</td>\n",
       "      <td>0.571040</td>\n",
       "      <td>0.583173</td>\n",
       "      <td>0.574167</td>\n",
       "      <td>0.581918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf_5_25_log2</td>\n",
       "      <td>0.582214</td>\n",
       "      <td>0.587574</td>\n",
       "      <td>0.569510</td>\n",
       "      <td>0.581299</td>\n",
       "      <td>0.573493</td>\n",
       "      <td>0.578818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_5_25_10</td>\n",
       "      <td>0.590721</td>\n",
       "      <td>0.589326</td>\n",
       "      <td>0.573610</td>\n",
       "      <td>0.587837</td>\n",
       "      <td>0.582451</td>\n",
       "      <td>0.584789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf_5_15_sqrt</td>\n",
       "      <td>0.607126</td>\n",
       "      <td>0.610540</td>\n",
       "      <td>0.590999</td>\n",
       "      <td>0.602567</td>\n",
       "      <td>0.587898</td>\n",
       "      <td>0.599826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf_5_15_log2</td>\n",
       "      <td>0.607051</td>\n",
       "      <td>0.603031</td>\n",
       "      <td>0.584913</td>\n",
       "      <td>0.602176</td>\n",
       "      <td>0.587622</td>\n",
       "      <td>0.596959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf_5_15_10</td>\n",
       "      <td>0.602851</td>\n",
       "      <td>0.605304</td>\n",
       "      <td>0.590477</td>\n",
       "      <td>0.605044</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.599547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rf_10_None_sqrt</td>\n",
       "      <td>0.600380</td>\n",
       "      <td>0.601241</td>\n",
       "      <td>0.585525</td>\n",
       "      <td>0.594863</td>\n",
       "      <td>0.587921</td>\n",
       "      <td>0.593986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf_10_None_log2</td>\n",
       "      <td>0.594928</td>\n",
       "      <td>0.599614</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>0.598863</td>\n",
       "      <td>0.585262</td>\n",
       "      <td>0.591923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rf_10_None_10</td>\n",
       "      <td>0.602325</td>\n",
       "      <td>0.600251</td>\n",
       "      <td>0.588655</td>\n",
       "      <td>0.597694</td>\n",
       "      <td>0.590751</td>\n",
       "      <td>0.595935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rf_10_25_sqrt</td>\n",
       "      <td>0.598848</td>\n",
       "      <td>0.601647</td>\n",
       "      <td>0.586665</td>\n",
       "      <td>0.595486</td>\n",
       "      <td>0.590249</td>\n",
       "      <td>0.594579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rf_10_25_log2</td>\n",
       "      <td>0.595846</td>\n",
       "      <td>0.600525</td>\n",
       "      <td>0.584063</td>\n",
       "      <td>0.598675</td>\n",
       "      <td>0.587716</td>\n",
       "      <td>0.593365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rf_10_25_10</td>\n",
       "      <td>0.603204</td>\n",
       "      <td>0.598404</td>\n",
       "      <td>0.585713</td>\n",
       "      <td>0.598113</td>\n",
       "      <td>0.592616</td>\n",
       "      <td>0.595610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rf_10_15_sqrt</td>\n",
       "      <td>0.614319</td>\n",
       "      <td>0.613578</td>\n",
       "      <td>0.599370</td>\n",
       "      <td>0.606911</td>\n",
       "      <td>0.598521</td>\n",
       "      <td>0.606540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf_10_15_log2</td>\n",
       "      <td>0.613170</td>\n",
       "      <td>0.616053</td>\n",
       "      <td>0.593112</td>\n",
       "      <td>0.610067</td>\n",
       "      <td>0.596998</td>\n",
       "      <td>0.605880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rf_10_15_10</td>\n",
       "      <td>0.611976</td>\n",
       "      <td>0.618164</td>\n",
       "      <td>0.601073</td>\n",
       "      <td>0.608310</td>\n",
       "      <td>0.600976</td>\n",
       "      <td>0.608100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier     Run 1     Run 2     Run 3     Run 4     Run 5  \\\n",
       "0    rf_2_None_sqrt  0.583218  0.578027  0.563582  0.575798  0.569615   \n",
       "1    rf_2_None_log2  0.574066  0.574220  0.564018  0.573494  0.568070   \n",
       "2      rf_2_None_10  0.587130  0.585437  0.566015  0.580056  0.579016   \n",
       "3      rf_2_25_sqrt  0.582239  0.580290  0.567900  0.584945  0.573208   \n",
       "4      rf_2_25_log2  0.576208  0.572987  0.561835  0.575825  0.567494   \n",
       "5        rf_2_25_10  0.585995  0.582536  0.563105  0.581095  0.577646   \n",
       "6      rf_2_15_sqrt  0.598159  0.596776  0.584242  0.597314  0.579315   \n",
       "7      rf_2_15_log2  0.606509  0.599328  0.583712  0.597333  0.584062   \n",
       "8        rf_2_15_10  0.605062  0.604182  0.586051  0.597718  0.588239   \n",
       "9    rf_5_None_sqrt  0.588709  0.590061  0.574136  0.582569  0.575039   \n",
       "10   rf_5_None_log2  0.582678  0.585487  0.565741  0.582234  0.574046   \n",
       "11     rf_5_None_10  0.586709  0.588021  0.575714  0.585916  0.586109   \n",
       "12     rf_5_25_sqrt  0.587815  0.593396  0.571040  0.583173  0.574167   \n",
       "13     rf_5_25_log2  0.582214  0.587574  0.569510  0.581299  0.573493   \n",
       "14       rf_5_25_10  0.590721  0.589326  0.573610  0.587837  0.582451   \n",
       "15     rf_5_15_sqrt  0.607126  0.610540  0.590999  0.602567  0.587898   \n",
       "16     rf_5_15_log2  0.607051  0.603031  0.584913  0.602176  0.587622   \n",
       "17       rf_5_15_10  0.602851  0.605304  0.590477  0.605044  0.594059   \n",
       "18  rf_10_None_sqrt  0.600380  0.601241  0.585525  0.594863  0.587921   \n",
       "19  rf_10_None_log2  0.594928  0.599614  0.580948  0.598863  0.585262   \n",
       "20    rf_10_None_10  0.602325  0.600251  0.588655  0.597694  0.590751   \n",
       "21    rf_10_25_sqrt  0.598848  0.601647  0.586665  0.595486  0.590249   \n",
       "22    rf_10_25_log2  0.595846  0.600525  0.584063  0.598675  0.587716   \n",
       "23      rf_10_25_10  0.603204  0.598404  0.585713  0.598113  0.592616   \n",
       "24    rf_10_15_sqrt  0.614319  0.613578  0.599370  0.606911  0.598521   \n",
       "25    rf_10_15_log2  0.613170  0.616053  0.593112  0.610067  0.596998   \n",
       "26      rf_10_15_10  0.611976  0.618164  0.601073  0.608310  0.600976   \n",
       "\n",
       "     Average  \n",
       "0   0.574048  \n",
       "1   0.570774  \n",
       "2   0.579531  \n",
       "3   0.577716  \n",
       "4   0.570870  \n",
       "5   0.578076  \n",
       "6   0.591161  \n",
       "7   0.594189  \n",
       "8   0.596250  \n",
       "9   0.582103  \n",
       "10  0.578037  \n",
       "11  0.584494  \n",
       "12  0.581918  \n",
       "13  0.578818  \n",
       "14  0.584789  \n",
       "15  0.599826  \n",
       "16  0.596959  \n",
       "17  0.599547  \n",
       "18  0.593986  \n",
       "19  0.591923  \n",
       "20  0.595935  \n",
       "21  0.594579  \n",
       "22  0.593365  \n",
       "23  0.595610  \n",
       "24  0.606540  \n",
       "25  0.605880  \n",
       "26  0.608100  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_result = cbb.cross_val(train, exclude_cols, clfs, cl_names, scoring='roc_auc')\n",
    "cross_val_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are a lot of results to process above, I found that some patterns emerged for each of the parameters we are tuning:\n",
    "* min_samples_split: There is the least amount of consensus for this parameter, It seems like all of the options I tried worked well with at least some combination of the others. However, since 2 is the default, I will generally favor using that value.\n",
    "* max_depth: It seems that generally the models with a larger maximum depth performance better. However, smaller and therefore simpler trees should be more robust, so I would like to use 25 as a maximum depth if possible.\n",
    "* max_features: Generally, the models with more features performed better. Since there are a good number of features in this data set and we have already worked to try and reduce that number as much as possible, I think it is alright to use up to ten features for the trees.\n",
    "\n",
    "Based on the trends above, I picked out some of the combinations of parameters to test them out on the training tournament data. Overall, I settled on using the default 2 for min_samples_split, a max depth of 25 and a max number of features of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_2_None_10</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.665851</td>\n",
       "      <td>0.180470</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_2_25_10</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.663833</td>\n",
       "      <td>0.180978</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_5_None_10</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.236025</td>\n",
       "      <td>0.664075</td>\n",
       "      <td>0.180483</td>\n",
       "      <td>0.726667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_10_25_10</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.667976</td>\n",
       "      <td>0.178581</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_5_None_sqrt</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.667492</td>\n",
       "      <td>0.179263</td>\n",
       "      <td>0.728889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_10_15_sqrt</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.209790</td>\n",
       "      <td>0.684307</td>\n",
       "      <td>0.173666</td>\n",
       "      <td>0.748889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classifier  Precision    Recall        F1       AUC     Brier  Accuracy\n",
       "0    rf_2_None_10   0.382979  0.165138  0.230769  0.665851  0.180470  0.733333\n",
       "1      rf_2_25_10   0.387755  0.174312  0.240506  0.663833  0.180978  0.733333\n",
       "2    rf_5_None_10   0.365385  0.174312  0.236025  0.664075  0.180483  0.726667\n",
       "3     rf_10_25_10   0.387755  0.174312  0.240506  0.667976  0.178581  0.733333\n",
       "4  rf_5_None_sqrt   0.348837  0.137615  0.197368  0.667492  0.179263  0.728889\n",
       "5   rf_10_15_sqrt   0.441176  0.137615  0.209790  0.684307  0.173666  0.748889"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_2_None_10 = RandomForestClassifier(n_estimators=500, min_samples_split=2, max_depth=None, max_features=10, random_state=77)\n",
    "clf_2_25_10 = RandomForestClassifier(n_estimators=500, min_samples_split=2, max_depth=25, max_features=10, random_state=77)\n",
    "clf_5_None_10 = RandomForestClassifier(n_estimators=500, min_samples_split=5, max_depth=None, max_features=10, random_state=77)\n",
    "clf_10_25_10 = RandomForestClassifier(n_estimators=500, min_samples_split=10, max_depth=25, max_features=10, random_state=77)\n",
    "clf_5_None_sqrt = RandomForestClassifier(n_estimators=500, min_samples_split=5, max_depth=None, max_features='sqrt', random_state=77)\n",
    "clf_10_15_sqrt = RandomForestClassifier(n_estimators=500, min_samples_split=10, max_depth=15, max_features='sqrt', random_state=77)\n",
    "\n",
    "clfs = [clf_2_None_10, clf_2_25_10, clf_5_None_10, clf_10_25_10, clf_5_None_sqrt, clf_10_15_sqrt]\n",
    "cl_names = ['rf_2_None_10', 'rf_2_25_10', 'rf_5_None_10', 'rf_10_25_10', 'rf_5_None_sqrt', 'rf_10_15_sqrt']\n",
    "cbb.evaluate(season_train, march_train, exclude_cols, clfs, cl_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Look at an XGBoost Model\n",
    "\n",
    "Lastly, we will also take a look at using an XGBoost model. I have the least amount of familiarity with this type of model, so I'm just going to stick with the default values for now. Regardless, we'll run some cross validation and see how the model performs when trained on the regular season training data and tested on the march training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Run 1</th>\n",
       "      <th>Run 2</th>\n",
       "      <th>Run 3</th>\n",
       "      <th>Run 4</th>\n",
       "      <th>Run 5</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.333874</td>\n",
       "      <td>0.343671</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.338141</td>\n",
       "      <td>0.350797</td>\n",
       "      <td>0.335983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classifier     Run 1     Run 2     Run 3     Run 4     Run 5   Average\n",
       "0    XGBoost  0.333874  0.343671  0.313433  0.338141  0.350797  0.335983"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(n_estimators=500, use_label_encoder=False, eval_metric='logloss', random_state=77)\n",
    "cbb.cross_val(train, exclude_cols, [clf], ['XGBoost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Run 1</th>\n",
       "      <th>Run 2</th>\n",
       "      <th>Run 3</th>\n",
       "      <th>Run 4</th>\n",
       "      <th>Run 5</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.587354</td>\n",
       "      <td>0.589086</td>\n",
       "      <td>0.57667</td>\n",
       "      <td>0.580571</td>\n",
       "      <td>0.584555</td>\n",
       "      <td>0.583647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classifier     Run 1     Run 2    Run 3     Run 4     Run 5   Average\n",
       "0    XGBoost  0.587354  0.589086  0.57667  0.580571  0.584555  0.583647"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbb.cross_val(train, exclude_cols, [clf], ['XGBoost'], scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.256881</td>\n",
       "      <td>0.284264</td>\n",
       "      <td>0.649358</td>\n",
       "      <td>0.227215</td>\n",
       "      <td>0.686667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classifier  Precision    Recall        F1       AUC     Brier  Accuracy\n",
       "0    XGBoost   0.318182  0.256881  0.284264  0.649358  0.227215  0.686667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbb.evaluate(season_train, march_train, exclude_cols, [clf], ['XGBoost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a ML Model\n",
    "\n",
    "Now that we've found the set of ideal parameters for each of the algorithms we'll be testing, we can compare the results of training each on the training data and then testing on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.343612</td>\n",
       "      <td>0.741713</td>\n",
       "      <td>0.166534</td>\n",
       "      <td>0.751667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.680888</td>\n",
       "      <td>0.183794</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>0.262821</td>\n",
       "      <td>0.312977</td>\n",
       "      <td>0.668933</td>\n",
       "      <td>0.221983</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Precision    Recall        F1       AUC     Brier  \\\n",
       "0  Logistic Regression   0.549296  0.250000  0.343612  0.741713  0.166534   \n",
       "1        Random Forest   0.391304  0.173077  0.240000  0.680888  0.183794   \n",
       "2              XGBoost   0.386792  0.262821  0.312977  0.668933  0.221983   \n",
       "\n",
       "   Accuracy  \n",
       "0  0.751667  \n",
       "1  0.715000  \n",
       "2  0.700000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the models\n",
    "log = LogisticRegression(penalty='l2', C=10, solver='liblinear', random_state=77)\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=25, max_features=10, random_state=77)\n",
    "xgb = XGBClassifier(n_estimators=500, use_label_encoder=False, eval_metric='logloss', random_state=77)\n",
    "\n",
    "clfs = [log, rf, xgb]\n",
    "cl_names = ['Logistic Regression', 'Random Forest', 'XGBoost']\n",
    "\n",
    "cbb.evaluate(season, march, exclude_cols, clfs, cl_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the Logistic Regression model is performing best when we look at precision, recall, f1 and AUC scores. Interestingly, the other models both have a higher Brier score, which could indicate that while they're prediction may be less accurate, they are generating better probability values.\n",
    "\n",
    "Before definitively selecting one of these models, I also want to drill down a little more specifically on how well the models are able to predict NCAA Tournament games. After all, that is the goal of this project. In order to gauge the models' accuracy for those games, I've developed a version of cross validation that I like to call \"leave march out\" cross validation. The idea is to leave the games from one year's NCAA Tournament aside per fold, train the models on the rest of the data and then test them on that year's set of tournament games. This way, each fold is using most of the data we have for training and testing on a smaller set of games that are all NCAA Tournament games. The output shows how well the model performed on the games from each year's NCAA Tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.686198</td>\n",
       "      <td>0.174717</td>\n",
       "      <td>0.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.197571</td>\n",
       "      <td>0.701493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>0.805970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.166909</td>\n",
       "      <td>0.776119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.856589</td>\n",
       "      <td>0.164499</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.150936</td>\n",
       "      <td>0.791045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.650943</td>\n",
       "      <td>0.172578</td>\n",
       "      <td>0.731343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.791765</td>\n",
       "      <td>0.150621</td>\n",
       "      <td>0.791045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.758503</td>\n",
       "      <td>0.163731</td>\n",
       "      <td>0.731343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Precision    Recall        F1       AUC     Brier  Accuracy\n",
       "Classifier                                                             \n",
       "2010         0.285714  0.125000  0.173913  0.686198  0.174717  0.703125\n",
       "2011         0.444444  0.210526  0.285714  0.664474  0.197571  0.701493\n",
       "2012         0.700000  0.411765  0.518519  0.788235  0.153985  0.805970\n",
       "2013         0.666667  0.235294  0.347826  0.735294  0.166909  0.776119\n",
       "2014         1.000000  0.291667  0.451613  0.856589  0.164499  0.746269\n",
       "2015         0.500000  0.285714  0.363636  0.716981  0.150936  0.791045\n",
       "2016         0.300000  0.214286  0.250000  0.650943  0.172578  0.731343\n",
       "2017         0.800000  0.235294  0.363636  0.791765  0.150621  0.791045\n",
       "2018         0.500000  0.222222  0.307692  0.758503  0.163731  0.731343"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results, log_data = cbb.leave_march_out_cv(season, march, exclude_cols, log)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.699219</td>\n",
       "      <td>0.171951</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.204803</td>\n",
       "      <td>0.716418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.728235</td>\n",
       "      <td>0.169946</td>\n",
       "      <td>0.776119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.594118</td>\n",
       "      <td>0.210633</td>\n",
       "      <td>0.701493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.840116</td>\n",
       "      <td>0.169989</td>\n",
       "      <td>0.776119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.700809</td>\n",
       "      <td>0.159999</td>\n",
       "      <td>0.791045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.551213</td>\n",
       "      <td>0.195736</td>\n",
       "      <td>0.701493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.737647</td>\n",
       "      <td>0.174721</td>\n",
       "      <td>0.686567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.177947</td>\n",
       "      <td>0.716418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Precision    Recall        F1       AUC     Brier  Accuracy\n",
       "Classifier                                                             \n",
       "2010         0.333333  0.062500  0.105263  0.699219  0.171951  0.734375\n",
       "2011         0.500000  0.210526  0.296296  0.631579  0.204803  0.716418\n",
       "2012         0.625000  0.294118  0.400000  0.728235  0.169946  0.776119\n",
       "2013         0.333333  0.176471  0.230769  0.594118  0.210633  0.701493\n",
       "2014         0.909091  0.416667  0.571429  0.840116  0.169989  0.776119\n",
       "2015         0.500000  0.214286  0.300000  0.700809  0.159999  0.791045\n",
       "2016         0.200000  0.142857  0.166667  0.551213  0.195736  0.701493\n",
       "2017         0.300000  0.176471  0.222222  0.737647  0.174721  0.686567\n",
       "2018         0.400000  0.111111  0.173913  0.690476  0.177947  0.716418"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results, rf_data = cbb.leave_march_out_cv(season, march, exclude_cols, rf)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.587240</td>\n",
       "      <td>0.258958</td>\n",
       "      <td>0.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.630482</td>\n",
       "      <td>0.244535</td>\n",
       "      <td>0.671642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.702353</td>\n",
       "      <td>0.186709</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.565882</td>\n",
       "      <td>0.273225</td>\n",
       "      <td>0.626866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.818798</td>\n",
       "      <td>0.164514</td>\n",
       "      <td>0.805970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.756065</td>\n",
       "      <td>0.168023</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.540431</td>\n",
       "      <td>0.249757</td>\n",
       "      <td>0.656716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712941</td>\n",
       "      <td>0.230898</td>\n",
       "      <td>0.671642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.699546</td>\n",
       "      <td>0.197052</td>\n",
       "      <td>0.731343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Precision    Recall        F1       AUC     Brier  Accuracy\n",
       "Classifier                                                             \n",
       "2010         0.111111  0.062500  0.080000  0.587240  0.258958  0.640625\n",
       "2011         0.400000  0.315789  0.352941  0.630482  0.244535  0.671642\n",
       "2012         0.500000  0.235294  0.320000  0.702353  0.186709  0.746269\n",
       "2013         0.166667  0.117647  0.137931  0.565882  0.273225  0.626866\n",
       "2014         0.866667  0.541667  0.666667  0.818798  0.164514  0.805970\n",
       "2015         0.363636  0.285714  0.320000  0.756065  0.168023  0.746269\n",
       "2016         0.200000  0.214286  0.206897  0.540431  0.249757  0.656716\n",
       "2017         0.307692  0.235294  0.266667  0.712941  0.230898  0.671642\n",
       "2018         0.500000  0.222222  0.307692  0.699546  0.197052  0.731343"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results, xgb_data = cbb.leave_march_out_cv(season, march, exclude_cols, xgb)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results support out finding from using the training and test sets - the logistic regression model is performing best by each metric except the Brier score.\n",
    "\n",
    "I think it's important to select a model that is able to generate useful prediction probabilities. Since the NCAA Tournament notoriously has so many upsets, I want to be sure that my model will predict enough upsets. To achieve this goal, I've found that in the past, I had to lower the threshold for an upset form the usual probability of 0.5, to something lower and in order for this technique to be effective, we need a model that is generating accurate probability values. \n",
    "\n",
    "One method to evaluate these probabilities is to use something like the Brier score used above or another similar method like Log Loss. However, I also wanted to use a more visual method to examine these probabilities closer. To do this, I used a binning method to group sets of games by the predicted probability of an upset assigned by a given model. Then I calculate the actual fraction of upsets in each bin and plotted the results. Ideally, if the model assigned a probability between 0.5 and 0.55 for a set of games, something like 55% of the games should have resulted in an upset. The closer that the resulting plots follow a slope of 1.0, the better it would appear that the model is predicting probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = [log_data['Probability'], rf_data['Probability'], xgb_data['Probability']]\n",
    "cbb.probability_graph(log_data['Label'], probs, cl_names, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the logistic regression and random forest models are producing better probabilities, which is directly at odds with the Brier scores we saw above. The chances a game is an upset seems to stay relatively level regardless of what the XGBoost model actually predicted as the predicted probability.\n",
    "\n",
    "Overall, I think I will go with the logistic regression model this year because it had the best performance by most metrics, is the simplest and seems to have relatively useful probability scores."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,markdown//md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
